{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJOPZNZmozTp+Mt6NuTPrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahar-mariam/marvel-level-1-report/blob/main/DecisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Task 9 : An introduction to Decision Trees**\n",
        "--------------------------------------------------------\n",
        "Decision Tree is a tree shaped diagram which is used to determine the course of action or the next step to be taken. It can be a decision, occurence or reaction.\n",
        "\n",
        "Decision Trees in ML:\n",
        "\n",
        "Decision trees are supervised learning structures used for classification and regression. They are tree-like structures with each node representing a decision, and each leaf representing a class label or continuous value.\n",
        "\n",
        "Decision trees solve 2 types of problems: Classification and Regression.\n",
        "When a Decision Tree classifies things or data, it is called Classification Tree\n",
        "When a Decision Tree predicts numerical values, it is called Regression Tree\n",
        "\n",
        "Decision tree algorithm is one of the most powerful classification and regression algorithms. It is easy to set up and can work with both continuous and categorical data.\n",
        "Decision trees tend to overfit, i.e., the model learns too much from the training data and does not learn well from new data. The best way to avoid overfitting is to prune a decision tree. Pruning is the process of cutting out branches in a decision tree that do not add to the model's accuracy.\n",
        "\n",
        "* Basically, the decision tree is made by dividing the data into different sub-data sets. At each sub-data set, the algorithm decides which feature to split it on based on an impurity measure.\n",
        "* This impurity measure tells the algorithm how good the data is at classifying it at that particular node.\n",
        "* The algorithm keeps splitting the data until it only has one class label per leaf node or a constant value. Then, the tree is ready to use to make predictions about new data.\n",
        "* To make a new data prediction, the algorithm begins at the root and moves down the tree's branches.\n",
        "* At each branch, it compares the feature's value to the threshold.\n",
        "*  If the feature's value is lower than the threshold value, the algorithm moves down the branch to the left.\n",
        "*   If it is greater than the threshold or equal to it, the algorithm moves up the branch to the right.\n",
        "* As the algorithm progresses down the tree, it moves from branch to branch until it reaches the leaf node.\n",
        "* At the leaf node, the class label or the continuous value represents the prediction of the new data.\n",
        "\n",
        "To use Decision Trees in a programming language the steps are:\n",
        "\n",
        "Present a dataset.\n",
        "Train a model, learning from descriptive features and a target feature.\n",
        "Continue the tree until accomplish a criteria.\n",
        "Create leaf nodes representing the predictions.\n",
        "Show instances and run down the tree until arrive at leaf nodes.\n",
        "  \n",
        "1. Start with a dataset of labeled data.\n",
        "2. Select the best feature to split the data on.\n",
        "3. Split the data into two subsets: one subset contains the data points that have the value of the feature that is being split on, and the other subset contains the data points that do not have that value.\n",
        "4. Recursively apply the same process to each subset until each subset contains data points that all belong to the same class or have the same value.\n",
        "5. The resulting tree is a decision tree model that can be used to classify or predict the value of new data points.\n",
        "\n",
        "  \n",
        "1. **Data Collection**. The first step is to collect data that is relevant to the problem you are trying to solve. This data can be in the form of text, images, or numbers.\n",
        "2. **Data Preprocessing**. Once you have collected data, you need to preprocess it so that it can be used by the decision tree algorithm. This may involve cleaning the data, removing duplicate data points, and converting the data into a format that the algorithm can understand.\n",
        "3. **Feature Selection**. The next step is to select the features that will be used to build the decision tree. This is an important step, as the features you select will determine the accuracy of the tree.\n",
        "4. **Tree Construction**. Once you have selected the features, you can start building the decision tree. This is done by recursively splitting the data into smaller and smaller subsets, until each subset contains only one type of data point.\n",
        "5. **Tree Pruning**. Once the tree has been built, you may need to prune it to improve its accuracy. This involves removing branches that are not important to the classification process.\n",
        "6. **Model Evaluation**. The final step is to evaluate the performance of the decision tree. This can be done by using a holdout dataset or by using cross-validation.\n",
        "   \n",
        "   Some important terms in Decision Trees:\n",
        "   1. Gini impurity:\n",
        "      The Gini impurity can be used to measure the impurity of a set of data for any number of classes. It is a useful measure of impurity because it is easy to calculate and it is not biased towards any particular class.\n",
        "      ![gini impurity](https://i.ibb.co/p2wHGym/gini-impurity.png)\n",
        "\n",
        "2. Gini Index: The Gini index is used as a measure of inequality.\n",
        "    The Gini index can range from 0 to 1. A Gini index of 0 represents perfect equality, while a Gini index of 1 represents perfect inequality.\n",
        "\n",
        "  \n",
        "*Decision Tree implementation*:\n",
        "https://github.com/sahar-mariam/marvel-level-1-report/blob/main/decision_tree.ipynb\n",
        "\n",
        "-------------------------------------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "eoSdlkPgq17n"
      }
    }
  ]
}